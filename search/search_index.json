{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Motivation With Panda-like dataframe or numby-like array, do you want to create a code, and choose at the end, the framework to use? Do you want to be able to choose the best framework after simply performing performance measurements? This framework unifies multiple Panda-compatible or Numpy-comptaible components, to allow the writing of a single code, compatible with all. This project will weave your code with the selected framework, at runtime. Synopsis With some parameters and Virtual classes, it's possible to write a code, and execute this code: With or without multicore With or without cluster (multi nodes on Dask or Spark) With or without GPU To do that, we create some virtual classes, add some methods in others classes, etc. To reduce the confusion, you must use the classes VDataFrame and VSeries (The prefix V is for Virtual ). These classes propose the methods .to_pandas() and .compute() for each version, but are the real classes of the selected framework. With some parameters, the real classes may be pandas.DataFrame , modin.pandas.DataFrame , cudf.DataFrame , pyspark.pandas.DataFrame without GPU, pyspark.pandas.DataFrame with GPU, dask.dataframe.DataFrame with Pandas or dask.dataframe.DataFrame with cudf (with Pandas or cudf for each partition). Or, for Numpy, the real classes may be numpy.ndarray , cupy.ndarray or dask.array . A new @delayed annotation can be use, with or without Dask. To manage the initialisation of a Dask ou Spark, you must use the VClient() , a connector to the cluster. This alias, can be automatically initialized with some environment variables. # Sample of code, compatible Pandas, cudf, dask, dask_modin and dask_cudf from virtual_dataframe import * TestDF = VDataFrame with (VClient()): @delayed def my_function(data: TestDF) -> TestDF: return data rc = my_function(VDataFrame({\"data\": [1, 2]}, npartitions=2)) print(rc.to_pandas()) With this framework, you can select your environment, to run or debug your code. env Environement VDF_MODE=pandas Only Python with classical pandas VDF_MODE=numpy Alias of pandas VDF_MODE=cudf Python with local cuDF (GPU) VDF_MODE=cupy Alias of cudf VDF_MODE=dask Dask with local multiple process and pandas VDF_MODE=dask_array Alias of dask VDF_MODE=dask_cudf Dask with local multiple process and cuDF VDF_MODE=dask DEBUG=True Dask with single thread and pandas VDF_MODE=dask_cudf DEBUG=True Dask with single thread and cuDF VDF_MODE=dask VDF_CLUSTER=dask://.local Dask with local cluster and pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://.local Dask with local cuda cluster and cuDF VDF_MODE=dask VDF_CLUSTER=dask://...:ppp Dask with remote cluster and Pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://...:ppp Dask with remote cluster and cuDF VDF_MODE=dask_modin Dask with modin VDF_MODE=dask_modin VDF_CLUSTER=dask://.local Dask with local cluster and modin VDF_MODE=dask_modin VDF_CLUSTER=dask://...:ppp Dask with remote cluster and modin VDF_MODE=pyspark VDF_CLUSTER=spark://.local PySpark with local cluster and modin VDF_MODE=pyspark VDF_CLUSTER=spark://...:ppp PySpark with remote cluster and modin For pyspark with GPU, read this . The real compatibilty between the differents simulation of Pandas, depends on the implement of the modin, cudf, pyspark or dask. Sometime, you can use the VDF_MODE variable, to update some part of code, between the selected backend. It's not always easy to write a code compatible with all scenario, but it's possible. Generally, add just .compute() and/or .to_pandas() at the end of the ETL, is enough. But, you must use, only the common feature with all frameworks. After this effort, it's possible to compare the performance about the differents technologies, or propose a component, compatible with differents scenario. For the deployment of your project, you can select the best framework for your process (in a dockerfile? or virtual environment), with only one ou two environment variables. May be, you can use Modin all the time, except for the end of the year periods, when the use of a GPU is preferable. With conda environment, you can use variables to set the variables when you activate an environment. Alternative The projet Fugue propose another approch to distribute a job with Spark, Dask or Ray.","title":"Home"},{"location":"#motivation","text":"With Panda-like dataframe or numby-like array, do you want to create a code, and choose at the end, the framework to use? Do you want to be able to choose the best framework after simply performing performance measurements? This framework unifies multiple Panda-compatible or Numpy-comptaible components, to allow the writing of a single code, compatible with all. This project will weave your code with the selected framework, at runtime.","title":"Motivation"},{"location":"#synopsis","text":"With some parameters and Virtual classes, it's possible to write a code, and execute this code: With or without multicore With or without cluster (multi nodes on Dask or Spark) With or without GPU To do that, we create some virtual classes, add some methods in others classes, etc. To reduce the confusion, you must use the classes VDataFrame and VSeries (The prefix V is for Virtual ). These classes propose the methods .to_pandas() and .compute() for each version, but are the real classes of the selected framework. With some parameters, the real classes may be pandas.DataFrame , modin.pandas.DataFrame , cudf.DataFrame , pyspark.pandas.DataFrame without GPU, pyspark.pandas.DataFrame with GPU, dask.dataframe.DataFrame with Pandas or dask.dataframe.DataFrame with cudf (with Pandas or cudf for each partition). Or, for Numpy, the real classes may be numpy.ndarray , cupy.ndarray or dask.array . A new @delayed annotation can be use, with or without Dask. To manage the initialisation of a Dask ou Spark, you must use the VClient() , a connector to the cluster. This alias, can be automatically initialized with some environment variables. # Sample of code, compatible Pandas, cudf, dask, dask_modin and dask_cudf from virtual_dataframe import * TestDF = VDataFrame with (VClient()): @delayed def my_function(data: TestDF) -> TestDF: return data rc = my_function(VDataFrame({\"data\": [1, 2]}, npartitions=2)) print(rc.to_pandas()) With this framework, you can select your environment, to run or debug your code. env Environement VDF_MODE=pandas Only Python with classical pandas VDF_MODE=numpy Alias of pandas VDF_MODE=cudf Python with local cuDF (GPU) VDF_MODE=cupy Alias of cudf VDF_MODE=dask Dask with local multiple process and pandas VDF_MODE=dask_array Alias of dask VDF_MODE=dask_cudf Dask with local multiple process and cuDF VDF_MODE=dask DEBUG=True Dask with single thread and pandas VDF_MODE=dask_cudf DEBUG=True Dask with single thread and cuDF VDF_MODE=dask VDF_CLUSTER=dask://.local Dask with local cluster and pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://.local Dask with local cuda cluster and cuDF VDF_MODE=dask VDF_CLUSTER=dask://...:ppp Dask with remote cluster and Pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://...:ppp Dask with remote cluster and cuDF VDF_MODE=dask_modin Dask with modin VDF_MODE=dask_modin VDF_CLUSTER=dask://.local Dask with local cluster and modin VDF_MODE=dask_modin VDF_CLUSTER=dask://...:ppp Dask with remote cluster and modin VDF_MODE=pyspark VDF_CLUSTER=spark://.local PySpark with local cluster and modin VDF_MODE=pyspark VDF_CLUSTER=spark://...:ppp PySpark with remote cluster and modin For pyspark with GPU, read this . The real compatibilty between the differents simulation of Pandas, depends on the implement of the modin, cudf, pyspark or dask. Sometime, you can use the VDF_MODE variable, to update some part of code, between the selected backend. It's not always easy to write a code compatible with all scenario, but it's possible. Generally, add just .compute() and/or .to_pandas() at the end of the ETL, is enough. But, you must use, only the common feature with all frameworks. After this effort, it's possible to compare the performance about the differents technologies, or propose a component, compatible with differents scenario. For the deployment of your project, you can select the best framework for your process (in a dockerfile? or virtual environment), with only one ou two environment variables. May be, you can use Modin all the time, except for the end of the year periods, when the use of a GPU is preferable. With conda environment, you can use variables to set the variables when you activate an environment.","title":"Synopsis"},{"location":"#alternative","text":"The projet Fugue propose another approch to distribute a job with Spark, Dask or Ray.","title":"Alternative"},{"location":"CHANGELOG/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [v0.6] - 2022-12-XX TODO Add support of Numpy frameworks (numpy, cupy, dask_array, dask_cupy) Added numpy , cupy , dask.array frameworks to_numpy() to_ndarray() FrontEndNumpy , BackEndNumpy import vdf.numpy vdf.numpy.array(..., chunks=...) vdf.numpy.asnumpy(d) vdf.numpy.asdarray(d) vdf.numpy.save(d) vdf.numpy.load(d) Changed Rename FrontEnd to FrontEndPandas Rename BackEnd to BackEndPandas [v0.5] - 2022-11-15 First version published on the repositories (pip and conda-forge). Added Pandas, cudf, modin, dask, dask_cudf, dask_modin, pyspark and pyspark+rapids frameworks Changed Nop Deprecated Nop Removed Nop Fixed Nop Security Nop","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#v06-2022-12-xx-todo","text":"Add support of Numpy frameworks (numpy, cupy, dask_array, dask_cupy)","title":"[v0.6] - 2022-12-XX TODO"},{"location":"CHANGELOG/#added","text":"numpy , cupy , dask.array frameworks to_numpy() to_ndarray() FrontEndNumpy , BackEndNumpy import vdf.numpy vdf.numpy.array(..., chunks=...) vdf.numpy.asnumpy(d) vdf.numpy.asdarray(d) vdf.numpy.save(d) vdf.numpy.load(d)","title":"Added"},{"location":"CHANGELOG/#changed","text":"Rename FrontEnd to FrontEndPandas Rename BackEnd to BackEndPandas","title":"Changed"},{"location":"CHANGELOG/#v05-2022-11-15","text":"First version published on the repositories (pip and conda-forge).","title":"[v0.5] - 2022-11-15"},{"location":"CHANGELOG/#added_1","text":"Pandas, cudf, modin, dask, dask_cudf, dask_modin, pyspark and pyspark+rapids frameworks","title":"Added"},{"location":"CHANGELOG/#changed_1","text":"Nop","title":"Changed"},{"location":"CHANGELOG/#deprecated","text":"Nop","title":"Deprecated"},{"location":"CHANGELOG/#removed","text":"Nop","title":"Removed"},{"location":"CHANGELOG/#fixed","text":"Nop","title":"Fixed"},{"location":"CHANGELOG/#security","text":"Nop","title":"Security"},{"location":"api/","text":"API api comments VDF_MODE The current mode Mode The enumeration of differents mode FrontEndPandas pandas , cudf , modin.pandas , dask_cudf , dask.dataframe or pyspark.pandas FrontEndNumpy numpy , cupy or dask.array BackEndDataFrame pandas or cudf BackEndSeries pandas.Series , cudf.Series or modin.pandas.Series BackEndNDArray numpy.ndarray or cupy.ndarray BackEndNumpy pandas , cudf or modin.pandas vdf.@delayed Delayed function (do nothing or dask.delayed) vdf.concat(...) Merge VDataFrame vdf.read_csv(...) Read VDataFrame from CSVs glob files vdf.read_excel(...) * Read VDataFrame from Excel glob files vdf.read_fwf(...) * Read VDataFrame from Fwf glob files vdf.read_hdf(...) * Read VDataFrame from HDFs glob files vdf.read_json(...) Read VDataFrame from Jsons glob files vdf.read_orc(...) Read VDataFrame from ORCs glob files vdf.read_parquet(...) Read VDataFrame from Parquets glob files vdf.read_sql_table(...) * Read VDataFrame from SQL vdf.from_pandas(pdf, npartitions=...) Create Virtual Dataframe from Pandas DataFrame vdf.from_backend(vdf, npartitions=...) Create Virtual Dataframe from backend dataframe vdf.compute([...]) Compute multiple @delayed functions VDataFrame(data, npartitions=...) Create DataFrame in memory (only for test) VSeries(data, npartitions=...) Create Series in memory (only for test) VLocalCluster(...) Create a dask Local Cluster (Dask, Cuda or Spark) VDataFrame.compute() Compute the virtual dataframe VDataFrame.persist() Persist the dataframe in memory VDataFrame.repartition() Rebalance the dataframe VDataFrame.visualize() Create an image with the graph VDataFrame.to_pandas() Convert to pandas dataframe VDataFrame.to_backend() Convert to backend dataframe VDataFrame.to_csv() Save to glob files VDataFrame.to_excel() * Save to glob files VDataFrame.to_feather() * Save to glob files VDataFrame.to_hdf() * Save to glob files VDataFrame.to_json() Save to glob files VDataFrame.to_orc() Save to glob files VDataFrame.to_parquet() Save to glob files VDataFrame.to_sql() * Save to sql table VDataFrame.to_numpy() Convert to numpy array VDataFrame.categorize() Detect all categories VDataFrame.apply_rows() Apply rows, GPU template VDataFrame.map_partitions() Apply function for each parttions VDataFrame.to_ndarray() Convert to numpy or cupy ndarray VDataFrame.to_numpy() Convert to numpy, cupy or dask array VSeries.compute() Compute the virtual series VSeries.persist() Persist the dataframe in memory VSeries.repartition() Rebalance the dataframe VSeries.visualize() Create an image with the graph VSeries.to_pandas() Convert to pandas series VSeries.to_backend() Convert to backend series (pandas, cudf, dask.Series) VSeries.to_numpy() Convert to numpy ndarray VSeries.to_ndarray() Convert to numpy backend (numpy, cupy or dask.array) VClient(...) The connexion with the cluster import vdf.numpy Import numpy, cupy or dask.array vdf.numpy.array(..., chunks=...) Create an numpy-like array vdf.numpy.arange(...) Return evenly spaced values within a given interval. vdf.numpy.zeros(...) Return a new array of given shape and type, filled with zeros. vdf.numpy.zeros_like(...) Return an array of zeros with the same shape and type as a given array. vdf.numpy.asnumpy(d) Convert to numpy.ndarray vdf.numpy.save(d) Save to npy format vdf.numpy.load(d) Load npy format vdf.numpy.random.xxx(..., chunks=...) Generate random values vdf.numpy.compute(ar) Similare of ar.compute() vdf.numpy.rechuck(df) Alias of `ar.rechuck() vdf.numpy.asnumpy(df) Return a numpy ndarray from a DataFrame vdf.numpy.asndarray(df) Return a ndarray from a DataFrame (may be be numpy, cupy or dask.array) * some frameworks do not implement it With numpy like framework, it's impossible to add some methods like .compute() to numpy.ndarray or cupy.ndarray . To be compatible, you must use a global simulare function like vnp.compute(a)[0] # in place of a.compute() . You can read a sample notebook here for Pandas or here for Numpy for an example of the use of API. Keep in mind, the current framework are in FrontEndPandas and FrontEndNumpy , and the backend API (use inside dask) are in BackEndPandas and BackEndNumpy . To maintain this relationship, use: .to_backend() in place of df.to_pandas() .to_ndarray() in place of df.to_numpy() vdf.numpy.asndarray(df) in place of numpy.asarray(df) The .to_backend() can return a Pandas or a Cudf dataframe. The .to_ndarray() can return a Numpy, a Cupy or a dask.array array. The .asndarray(...) can return a Numpy, a Cupy or a dask.array array. Each API propose a specific version for each framework. For example: the toPandas() with Panda, return self @delayed use the dask @delayed or do nothing, and apply the code when the function was called. In the first case, the function return a part of the dask graph. In the second case, the function return immediately the result. read_csv(\"file*.csv\") read each file in parallele with dask, spark or pyspark by each worker, or read sequencially each file and combine each dataframe with a framework without distribution (pandas, modin, cudf) save_csv(\"file*.csv\") write each file in parallele with dask, spark or pyspark, or write one file \"file.csv\" (without the star) with a framework without distribution (pandas, modin, cudf) ... All adjustement was described here","title":"API"},{"location":"api/#api","text":"api comments VDF_MODE The current mode Mode The enumeration of differents mode FrontEndPandas pandas , cudf , modin.pandas , dask_cudf , dask.dataframe or pyspark.pandas FrontEndNumpy numpy , cupy or dask.array BackEndDataFrame pandas or cudf BackEndSeries pandas.Series , cudf.Series or modin.pandas.Series BackEndNDArray numpy.ndarray or cupy.ndarray BackEndNumpy pandas , cudf or modin.pandas vdf.@delayed Delayed function (do nothing or dask.delayed) vdf.concat(...) Merge VDataFrame vdf.read_csv(...) Read VDataFrame from CSVs glob files vdf.read_excel(...) * Read VDataFrame from Excel glob files vdf.read_fwf(...) * Read VDataFrame from Fwf glob files vdf.read_hdf(...) * Read VDataFrame from HDFs glob files vdf.read_json(...) Read VDataFrame from Jsons glob files vdf.read_orc(...) Read VDataFrame from ORCs glob files vdf.read_parquet(...) Read VDataFrame from Parquets glob files vdf.read_sql_table(...) * Read VDataFrame from SQL vdf.from_pandas(pdf, npartitions=...) Create Virtual Dataframe from Pandas DataFrame vdf.from_backend(vdf, npartitions=...) Create Virtual Dataframe from backend dataframe vdf.compute([...]) Compute multiple @delayed functions VDataFrame(data, npartitions=...) Create DataFrame in memory (only for test) VSeries(data, npartitions=...) Create Series in memory (only for test) VLocalCluster(...) Create a dask Local Cluster (Dask, Cuda or Spark) VDataFrame.compute() Compute the virtual dataframe VDataFrame.persist() Persist the dataframe in memory VDataFrame.repartition() Rebalance the dataframe VDataFrame.visualize() Create an image with the graph VDataFrame.to_pandas() Convert to pandas dataframe VDataFrame.to_backend() Convert to backend dataframe VDataFrame.to_csv() Save to glob files VDataFrame.to_excel() * Save to glob files VDataFrame.to_feather() * Save to glob files VDataFrame.to_hdf() * Save to glob files VDataFrame.to_json() Save to glob files VDataFrame.to_orc() Save to glob files VDataFrame.to_parquet() Save to glob files VDataFrame.to_sql() * Save to sql table VDataFrame.to_numpy() Convert to numpy array VDataFrame.categorize() Detect all categories VDataFrame.apply_rows() Apply rows, GPU template VDataFrame.map_partitions() Apply function for each parttions VDataFrame.to_ndarray() Convert to numpy or cupy ndarray VDataFrame.to_numpy() Convert to numpy, cupy or dask array VSeries.compute() Compute the virtual series VSeries.persist() Persist the dataframe in memory VSeries.repartition() Rebalance the dataframe VSeries.visualize() Create an image with the graph VSeries.to_pandas() Convert to pandas series VSeries.to_backend() Convert to backend series (pandas, cudf, dask.Series) VSeries.to_numpy() Convert to numpy ndarray VSeries.to_ndarray() Convert to numpy backend (numpy, cupy or dask.array) VClient(...) The connexion with the cluster import vdf.numpy Import numpy, cupy or dask.array vdf.numpy.array(..., chunks=...) Create an numpy-like array vdf.numpy.arange(...) Return evenly spaced values within a given interval. vdf.numpy.zeros(...) Return a new array of given shape and type, filled with zeros. vdf.numpy.zeros_like(...) Return an array of zeros with the same shape and type as a given array. vdf.numpy.asnumpy(d) Convert to numpy.ndarray vdf.numpy.save(d) Save to npy format vdf.numpy.load(d) Load npy format vdf.numpy.random.xxx(..., chunks=...) Generate random values vdf.numpy.compute(ar) Similare of ar.compute() vdf.numpy.rechuck(df) Alias of `ar.rechuck() vdf.numpy.asnumpy(df) Return a numpy ndarray from a DataFrame vdf.numpy.asndarray(df) Return a ndarray from a DataFrame (may be be numpy, cupy or dask.array) * some frameworks do not implement it With numpy like framework, it's impossible to add some methods like .compute() to numpy.ndarray or cupy.ndarray . To be compatible, you must use a global simulare function like vnp.compute(a)[0] # in place of a.compute() . You can read a sample notebook here for Pandas or here for Numpy for an example of the use of API. Keep in mind, the current framework are in FrontEndPandas and FrontEndNumpy , and the backend API (use inside dask) are in BackEndPandas and BackEndNumpy . To maintain this relationship, use: .to_backend() in place of df.to_pandas() .to_ndarray() in place of df.to_numpy() vdf.numpy.asndarray(df) in place of numpy.asarray(df) The .to_backend() can return a Pandas or a Cudf dataframe. The .to_ndarray() can return a Numpy, a Cupy or a dask.array array. The .asndarray(...) can return a Numpy, a Cupy or a dask.array array. Each API propose a specific version for each framework. For example: the toPandas() with Panda, return self @delayed use the dask @delayed or do nothing, and apply the code when the function was called. In the first case, the function return a part of the dask graph. In the second case, the function return immediately the result. read_csv(\"file*.csv\") read each file in parallele with dask, spark or pyspark by each worker, or read sequencially each file and combine each dataframe with a framework without distribution (pandas, modin, cudf) save_csv(\"file*.csv\") write each file in parallele with dask, spark or pyspark, or write one file \"file.csv\" (without the star) with a framework without distribution (pandas, modin, cudf) ... All adjustement was described here","title":"API"},{"location":"bench/","text":"Bench To make a bench between framework, you must identify three step: - Time to start the local cluster , if any. - Time to compile the first time, the python code to C or GPU - Time to run the performance tests Our recommandation it to run one time your test, and only after, run multiple time the same performance tests and calculate the timeit magic command. The Python 3.11 need the same approach.","title":"Bench"},{"location":"bench/#bench","text":"To make a bench between framework, you must identify three step: - Time to start the local cluster , if any. - Time to compile the first time, the python code to C or GPU - Time to run the performance tests Our recommandation it to run one time your test, and only after, run multiple time the same performance tests and calculate the timeit magic command. The Python 3.11 need the same approach.","title":"Bench"},{"location":"best_practices/","text":"Best practices For write a code, optimized with all frameworks, you must use some best practices . Avoid very large partition: so that they fit in a worker's available memory. Avoid very large graphs: because that can create an overhead on task. Learn techniques for customization: in order to improve the efficiency of your processes. Stop using Dask when no longer needed: like when you are iterating on a much smaller amount of data ( to_backend() . Persist in distributed RAM when you can: in doing so, accessing RAM memory will be faster. Processes and threads: be careful to separate numeric work from text data to maintain efficiency. Load data with Dask or Spark : for instance, if you need to work with large Python objects, let Dask create them (instead of creating them outside Dask or Spark and then handing thom over the framework). Avoid using VDataFrame or VSeries outside the unit tests Avoid calling compute repeatedly: as this can lower performance. Avoid iterate over rows or columns of DataFrame: Use apply() or apply_rows() to distribute the code in the cluster and GPU. If you know that the volume of data is compatible with one node, you can convert a distributed dataframe to BackendDataFrame and continue to manipulate the data locally. Use the .to_backend() to convert a Dataframe to a local Pandas like Dataframe or .to_ndarray() to convert a DataFrame to a local numpy like array.","title":"Best Practice"},{"location":"best_practices/#best-practices","text":"For write a code, optimized with all frameworks, you must use some best practices . Avoid very large partition: so that they fit in a worker's available memory. Avoid very large graphs: because that can create an overhead on task. Learn techniques for customization: in order to improve the efficiency of your processes. Stop using Dask when no longer needed: like when you are iterating on a much smaller amount of data ( to_backend() . Persist in distributed RAM when you can: in doing so, accessing RAM memory will be faster. Processes and threads: be careful to separate numeric work from text data to maintain efficiency. Load data with Dask or Spark : for instance, if you need to work with large Python objects, let Dask create them (instead of creating them outside Dask or Spark and then handing thom over the framework). Avoid using VDataFrame or VSeries outside the unit tests Avoid calling compute repeatedly: as this can lower performance. Avoid iterate over rows or columns of DataFrame: Use apply() or apply_rows() to distribute the code in the cluster and GPU. If you know that the volume of data is compatible with one node, you can convert a distributed dataframe to BackendDataFrame and continue to manipulate the data locally. Use the .to_backend() to convert a Dataframe to a local Pandas like Dataframe or .to_ndarray() to convert a DataFrame to a local numpy like array.","title":"Best practices"},{"location":"cluster/","text":"Cluster To connect to a cluster, use VDF_CLUSTER with protocol, host and optionaly, the port. dask://locahost:8787 spark://locahost:7077 or alternativelly, use DASK_SCHEDULER_SERVICE_HOST and DASK_SCHEDULER_SERVICE_PORT or SPARK_MASTER_HOST and SPARK_MASTER_PORT VDF_MODE DEBUG VDF_CLUSTER Scheduler pandas - - No scheduler cudf - - No scheduler modin - - No scheduler dask Yes - synchronous dask No - thread dask No dask://threads thread dask No dask://processes processes dask No dask://.local LocalCluster dask_modin No - LocalCluster dask_modin No dask://.local LocalCluster dask_modin No dask://<host>:<port> Dask cluster dask_cudf No dask://.local LocalCUDACluster dask_cudf No dask://<host>:<port> Dask cluster pyspark No spark:local[*] Spark local cluster pyspark No spark://.local Spark local cluster pyspark No spark://<host>:<port> Spark cluster The special host name , ends with .local can be used to start a LocalCluster , LocalCUDACluster or Spark local[*] when your program is started. An instance of local cluster is started and injected in the Client . Sample: from virtual_dataframe import VClient with VClient(): # Now, use the scheduler pass If you want to manage the parameters of Local(CUDA)Cluster or SparkCluster , use the alternative VLocalCluster() . from virtual_dataframe import VClient,VLocalCluster with VClient(VLocalCluster(params=...)): # Now, use the scheduler pass Dask local cluster To update the parameters for the implicit Local(CUDA)Cluster , you can use the Dask config file . local: scheduler-port: 0 device_memory_limit: 5G you can set some environment variables for dask, export DASK_LOCAL__SCHEDULER_PORT=0 export DASK_LOCAL__DEVICE_MEMORY_LIMIT=5g or for Domino datalab, export DASK_SCHEDULER_SERVICE_HOST=... export DASK_SCHEDULER_SERVICE_PORT=7077 Spark cluster To configure the spark cluster, - use a file spark.conf with the Spark properties use environment variables like export spark.app.name=MyApp . for VLocalCluster , use classical parameters, and replace dot to _ : from virtual_dataframe import VClient,VLocalCluster with VClient(VLocalCluster( spark_app_name=\"MyApp\", spark_master=\"local[*]\", )): # Now, use the scheduler pass or for Domino datalab, export SPARK_MASTER_HOST=... export SPARK_MASTER_PORT=7077 Spark cluster with GPU To use the Spark+rapids , download the file rapids-4-spark_2.12-22.10.0.jar (see here ). Then, in the file spark.conf , add: spark.jars=rapids-4-spark_2.12-22.10.0.jar spark.plugins=com.nvidia.spark.SQLPlugin spark.rapids.sql.concurrentGpuTasks=1","title":"Cluster"},{"location":"cluster/#cluster","text":"To connect to a cluster, use VDF_CLUSTER with protocol, host and optionaly, the port. dask://locahost:8787 spark://locahost:7077 or alternativelly, use DASK_SCHEDULER_SERVICE_HOST and DASK_SCHEDULER_SERVICE_PORT or SPARK_MASTER_HOST and SPARK_MASTER_PORT VDF_MODE DEBUG VDF_CLUSTER Scheduler pandas - - No scheduler cudf - - No scheduler modin - - No scheduler dask Yes - synchronous dask No - thread dask No dask://threads thread dask No dask://processes processes dask No dask://.local LocalCluster dask_modin No - LocalCluster dask_modin No dask://.local LocalCluster dask_modin No dask://<host>:<port> Dask cluster dask_cudf No dask://.local LocalCUDACluster dask_cudf No dask://<host>:<port> Dask cluster pyspark No spark:local[*] Spark local cluster pyspark No spark://.local Spark local cluster pyspark No spark://<host>:<port> Spark cluster The special host name , ends with .local can be used to start a LocalCluster , LocalCUDACluster or Spark local[*] when your program is started. An instance of local cluster is started and injected in the Client . Sample: from virtual_dataframe import VClient with VClient(): # Now, use the scheduler pass If you want to manage the parameters of Local(CUDA)Cluster or SparkCluster , use the alternative VLocalCluster() . from virtual_dataframe import VClient,VLocalCluster with VClient(VLocalCluster(params=...)): # Now, use the scheduler pass","title":"Cluster"},{"location":"cluster/#dask-local-cluster","text":"To update the parameters for the implicit Local(CUDA)Cluster , you can use the Dask config file . local: scheduler-port: 0 device_memory_limit: 5G you can set some environment variables for dask, export DASK_LOCAL__SCHEDULER_PORT=0 export DASK_LOCAL__DEVICE_MEMORY_LIMIT=5g or for Domino datalab, export DASK_SCHEDULER_SERVICE_HOST=... export DASK_SCHEDULER_SERVICE_PORT=7077","title":"Dask local cluster"},{"location":"cluster/#spark-cluster","text":"To configure the spark cluster, - use a file spark.conf with the Spark properties use environment variables like export spark.app.name=MyApp . for VLocalCluster , use classical parameters, and replace dot to _ : from virtual_dataframe import VClient,VLocalCluster with VClient(VLocalCluster( spark_app_name=\"MyApp\", spark_master=\"local[*]\", )): # Now, use the scheduler pass or for Domino datalab, export SPARK_MASTER_HOST=... export SPARK_MASTER_PORT=7077","title":"Spark cluster"},{"location":"cluster/#spark-cluster-with-gpu","text":"To use the Spark+rapids , download the file rapids-4-spark_2.12-22.10.0.jar (see here ). Then, in the file spark.conf , add: spark.jars=rapids-4-spark_2.12-22.10.0.jar spark.plugins=com.nvidia.spark.SQLPlugin spark.rapids.sql.concurrentGpuTasks=1","title":"Spark cluster with GPU"},{"location":"commands/","text":"Make commands The Makefile contains the central entry points for common tasks related to this project. Commands make help will print all majors target make configure will prepare the environment (conda venv, kernel, ...) make lint will lint the code make test will run all tests make typing will check the typing make validate will validate the version before commit make clean will clean current environment make docs will create and show a HTML documentation in 'build/' make dist will create a full wheel distribution Jupyter commands make notebook will start a jupyter notebook make remove-kernel will remove the project's kernel make clean-notebooks will clean all datas in the notebooks Twine commands make check-twine will check the packaging before publication make test-twine will publish the package in test.pypi.org <https://test.pypi.org> _) make twine will publish the package in pypi.org <https://pypi.org> _) Conda commands make conda-build will build the conda package make conda-debug will build the package in debug mode make conda-convert will convert the package for all platform make conda-install will install the conda package make conda-purge will remove the build artefact make conda-create will create a test environment for each mode make test-conda-forge will test the conda-forge first step publication make conda-forge will publish the project to conda-forge","title":"Commands"},{"location":"commands/#make-commands","text":"The Makefile contains the central entry points for common tasks related to this project.","title":"Make commands"},{"location":"commands/#commands","text":"make help will print all majors target make configure will prepare the environment (conda venv, kernel, ...) make lint will lint the code make test will run all tests make typing will check the typing make validate will validate the version before commit make clean will clean current environment make docs will create and show a HTML documentation in 'build/' make dist will create a full wheel distribution","title":"Commands"},{"location":"commands/#jupyter-commands","text":"make notebook will start a jupyter notebook make remove-kernel will remove the project's kernel make clean-notebooks will clean all datas in the notebooks","title":"Jupyter commands"},{"location":"commands/#twine-commands","text":"make check-twine will check the packaging before publication make test-twine will publish the package in test.pypi.org <https://test.pypi.org> _) make twine will publish the package in pypi.org <https://pypi.org> _)","title":"Twine commands"},{"location":"commands/#conda-commands","text":"make conda-build will build the conda package make conda-debug will build the package in debug mode make conda-convert will convert the package for all platform make conda-install will install the conda package make conda-purge will remove the build artefact make conda-create will create a test environment for each mode make test-conda-forge will test the conda-forge first step publication make conda-forge will publish the project to conda-forge","title":"Conda commands"},{"location":"compatibility/","text":"Compatibility This project is just a wrapper. So, it inherits limitations and bugs from other projects. Sorry for that. Limitations of Pandas like framework pandas All data must be in DRAM modin Read this cudf All data must be in VRAM All data types in cuDF are nullable Iterating over a cuDF Series, DataFrame or Index is not supported. Join (or merge) and groupby operations in cuDF do not guarantee output ordering. The order of operations is not always deterministic Cudf does not support duplicate column names Cudf also supports .apply() it relies on Numba to JIT compile the UDF and execute it on the GPU .apply(result_type=...) not supported dask transpose() and MultiIndex are not implemented Column assignment doesn't support type list dask_cudf See cudf and dask. Categories with strings not implemented pyspark Read this Limitations of Numpy like framework numpy All data must be in RAM cupy Read this - block() not implemented - delete() not implemented - insert() not implemented dask array Read this - identity() not implemented - asfarray() not implemented - asfortranarray() not implemented - ascontiguousarray() not implemented - asarray_chkfinite() not implemented - require() not implemented - column_stack() not implemented - row_stack() not implemented - *split*() not implemented - resize() not implemented - trim_zeros() not implemented - in1d() not implemented - intersect1d() not implemented - setdiff1d() not implemented - setxor1d() not implemented - column_stack() not implemented - row_stack() not implemented - fromiter() not implemented For compatibility between numpy and cupy, see here . File format compatibility To be compatible with all framework, you must only use the common features. We accept some function to read or write files, but we write a warning if you use a function not compatible with others frameworks. read_... / to_... pandas cudf modin dask dask_modin dask_cudf pyspark vdf.read_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_excel \u2713 \u2713 \u2713 VDataFrame.to_excel \u2713 \u2713 \u2713 VSeries.to_excel \u2713 \u2713 \u2713 vdf.read_feather \u2713 \u2713 \u2713 VDataFrame.to_feather \u2713 \u2713 \u2713 vdf.read_fwf \u2713 \u2713 \u2713 \u2713 vdf.read_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_sql_table \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_sql \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_sql \u2713 \u2713 \u2713 \u2713 \u2713 load... / save... numpy cupy dask.array vpd.load() npy \u2713 \u2713 \u2713 vpd.save() npy \u2713 \u2713 \u2713 vpd.savez() npz \u2713 \u2713 vpd.loadtxt() \u2713 \u2713 vpd.savetxt() \u2713 \u2713 Cross framework compatibility small data middle data big data 1-CPU pandas, numpy Limits:+ n-CPU modin, numpy Limits+ dask, dask_modin or pyspark and dask.array Limits:++ GPU cudf, cupy Limits:++ dask_cudf, pyspark+spark-rapids and dask.array Limits:+++ To develop, you can choose the level to be compatible with others frameworks. Each cell is strongly compatible with the upper left part. No need of GPU? If you don't need to use a GPU, then develop for dask and use mode in bold . small data middle data big data 1-CPU pandas, numpy Limits:+ n-CPU modin, numpy Limits+ dask, dask_modin or pyspark and dask.array Limits:++ GPU cudf, cupy Limits:++ dask_cudf, pyspark+spark-rapids and dask.array Limits:+++ You can ignore this API: VDataFrame.apply_rows() No need of big data? If you don't need to use big data, then develop for cudf and use mode in bold .. small data middle data big data 1-CPU pandas, numpy Limits:+ n-CPU modin, numpy Limits+ dask, dask_modin or pyspark and dask.array Limits:++ GPU cudf, cupy Limits:++ dask_cudf, pyspark+spark-rapids and dask.array Limits:+++ You can ignore these API: @delayed map_partitions() categorize() compute() npartitions=... Need all possibility? To be compatible with all modes, develop for dask_cudf . small data middle data big data 1-CPU pandas, numpy Limits:+ n-CPU modin, numpy Limits+ dask, dask_modin or pyspark and dask.array Limits:++ GPU cudf, cupy Limits:++ dask_cudf, pyspark+spark-rapids and dask.array Limits:+++ and accept all the limitations.","title":"Compatibility"},{"location":"compatibility/#compatibility","text":"This project is just a wrapper. So, it inherits limitations and bugs from other projects. Sorry for that. Limitations of Pandas like framework pandas All data must be in DRAM modin Read this cudf All data must be in VRAM All data types in cuDF are nullable Iterating over a cuDF Series, DataFrame or Index is not supported. Join (or merge) and groupby operations in cuDF do not guarantee output ordering. The order of operations is not always deterministic Cudf does not support duplicate column names Cudf also supports .apply() it relies on Numba to JIT compile the UDF and execute it on the GPU .apply(result_type=...) not supported dask transpose() and MultiIndex are not implemented Column assignment doesn't support type list dask_cudf See cudf and dask. Categories with strings not implemented pyspark Read this Limitations of Numpy like framework numpy All data must be in RAM cupy Read this - block() not implemented - delete() not implemented - insert() not implemented dask array Read this - identity() not implemented - asfarray() not implemented - asfortranarray() not implemented - ascontiguousarray() not implemented - asarray_chkfinite() not implemented - require() not implemented - column_stack() not implemented - row_stack() not implemented - *split*() not implemented - resize() not implemented - trim_zeros() not implemented - in1d() not implemented - intersect1d() not implemented - setdiff1d() not implemented - setxor1d() not implemented - column_stack() not implemented - row_stack() not implemented - fromiter() not implemented For compatibility between numpy and cupy, see here .","title":"Compatibility"},{"location":"compatibility/#file-format-compatibility","text":"To be compatible with all framework, you must only use the common features. We accept some function to read or write files, but we write a warning if you use a function not compatible with others frameworks. read_... / to_... pandas cudf modin dask dask_modin dask_cudf pyspark vdf.read_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_excel \u2713 \u2713 \u2713 VDataFrame.to_excel \u2713 \u2713 \u2713 VSeries.to_excel \u2713 \u2713 \u2713 vdf.read_feather \u2713 \u2713 \u2713 VDataFrame.to_feather \u2713 \u2713 \u2713 vdf.read_fwf \u2713 \u2713 \u2713 \u2713 vdf.read_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_sql_table \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_sql \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_sql \u2713 \u2713 \u2713 \u2713 \u2713 load... / save... numpy cupy dask.array vpd.load() npy \u2713 \u2713 \u2713 vpd.save() npy \u2713 \u2713 \u2713 vpd.savez() npz \u2713 \u2713 vpd.loadtxt() \u2713 \u2713 vpd.savetxt() \u2713 \u2713","title":"File format compatibility"},{"location":"compatibility/#cross-framework-compatibility","text":"small data middle data big data 1-CPU pandas, numpy Limits:+ n-CPU modin, numpy Limits+ dask, dask_modin or pyspark and dask.array Limits:++ GPU cudf, cupy Limits:++ dask_cudf, pyspark+spark-rapids and dask.array Limits:+++ To develop, you can choose the level to be compatible with others frameworks. Each cell is strongly compatible with the upper left part.","title":"Cross framework compatibility"},{"location":"compatibility/#no-need-of-gpu","text":"If you don't need to use a GPU, then develop for dask and use mode in bold . small data middle data big data 1-CPU pandas, numpy Limits:+ n-CPU modin, numpy Limits+ dask, dask_modin or pyspark and dask.array Limits:++ GPU cudf, cupy Limits:++ dask_cudf, pyspark+spark-rapids and dask.array Limits:+++ You can ignore this API: VDataFrame.apply_rows()","title":"No need of GPU?"},{"location":"compatibility/#no-need-of-big-data","text":"If you don't need to use big data, then develop for cudf and use mode in bold .. small data middle data big data 1-CPU pandas, numpy Limits:+ n-CPU modin, numpy Limits+ dask, dask_modin or pyspark and dask.array Limits:++ GPU cudf, cupy Limits:++ dask_cudf, pyspark+spark-rapids and dask.array Limits:+++ You can ignore these API: @delayed map_partitions() categorize() compute() npartitions=...","title":"No need of big data?"},{"location":"compatibility/#need-all-possibility","text":"To be compatible with all modes, develop for dask_cudf . small data middle data big data 1-CPU pandas, numpy Limits:+ n-CPU modin, numpy Limits+ dask, dask_modin or pyspark and dask.array Limits:++ GPU cudf, cupy Limits:++ dask_cudf, pyspark+spark-rapids and dask.array Limits:+++ and accept all the limitations.","title":"Need all possibility?"},{"location":"contribute/","text":"Contribute Get the latest version Clone the git repository $ git clone --recurse-submodules https://github.com/pprados/virtual-dataframe.git Installation Go inside the directory and $ make configure $ conda activate virtual_dataframe $ make Tests To test the project $ make test To validate the typing $ make typing To validate all the project $ make validate Project Organization \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 docs <- A default mkdocs \u251c\u2500\u2500 conda_recipe <- Script to build the conda package \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .[tests]) \u2502 so sources can be imported and dependencies installed \u251c\u2500\u2500 virtual_dataframe <- Source code for use in this project \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2514\u2500\u2500 *.py <- Framework codes \u2502 \u2514\u2500\u2500 tests <- Unit and integrations tests ((Mark directory as a sources root).","title":"Contribute"},{"location":"contribute/#contribute","text":"","title":"Contribute"},{"location":"contribute/#get-the-latest-version","text":"Clone the git repository $ git clone --recurse-submodules https://github.com/pprados/virtual-dataframe.git","title":"Get the latest version"},{"location":"contribute/#installation","text":"Go inside the directory and $ make configure $ conda activate virtual_dataframe $ make","title":"Installation"},{"location":"contribute/#tests","text":"To test the project $ make test To validate the typing $ make typing To validate all the project $ make validate","title":"Tests"},{"location":"contribute/#project-organization","text":"\u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 docs <- A default mkdocs \u251c\u2500\u2500 conda_recipe <- Script to build the conda package \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .[tests]) \u2502 so sources can be imported and dependencies installed \u251c\u2500\u2500 virtual_dataframe <- Source code for use in this project \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2514\u2500\u2500 *.py <- Framework codes \u2502 \u2514\u2500\u2500 tests <- Unit and integrations tests ((Mark directory as a sources root).","title":"Project Organization"},{"location":"faq/","text":"FAQ The code run with X, but not with Y? You must use only the similar functionality, and only a subpart of Pandas. Develop for dask_cudf . it's easier to be compatible with others frameworks. .compute() is not defined with pandas? If your @delayed function return something, other than a VDataFrame or VSerie , the objet does not have the method .compute() . You can solve this, with: @delayed def f()-> int: return 42 real_result,=compute(f()) # Warning, compute return a tuple. The comma is important. a,b = compute(f(),f()) .compute() is not defined with numpy? It's impossible to add this method inside the classes numpy.ndarray or cupy.ndarray . In place, use the global function compute(...) . ar=vnp.array([1,2,3]) compute(ar)[0] With CUDA, I receive NVMLError_NoPermission It's a problem with Dask. You have not the privilege to ask the size of memory for the GPU. To resolve that, add in dask configuration files : the parameter distributed:diagnostics:nvml = False the parameter local:device_memory_limit = 5g or the parameter --device-memory-limit 5g when you start dask-cuda-worker (update the size)","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#the-code-run-with-x-but-not-with-y","text":"You must use only the similar functionality, and only a subpart of Pandas. Develop for dask_cudf . it's easier to be compatible with others frameworks.","title":"The code run with X, but not with Y?"},{"location":"faq/#compute-is-not-defined-with-pandas","text":"If your @delayed function return something, other than a VDataFrame or VSerie , the objet does not have the method .compute() . You can solve this, with: @delayed def f()-> int: return 42 real_result,=compute(f()) # Warning, compute return a tuple. The comma is important. a,b = compute(f(),f())","title":".compute() is not defined with pandas?"},{"location":"faq/#compute-is-not-defined-with-numpy","text":"It's impossible to add this method inside the classes numpy.ndarray or cupy.ndarray . In place, use the global function compute(...) . ar=vnp.array([1,2,3]) compute(ar)[0]","title":".compute() is not defined with numpy?"},{"location":"faq/#with-cuda-i-receive-nvmlerror_nopermission","text":"It's a problem with Dask. You have not the privilege to ask the size of memory for the GPU. To resolve that, add in dask configuration files : the parameter distributed:diagnostics:nvml = False the parameter local:device_memory_limit = 5g or the parameter --device-memory-limit 5g when you start dask-cuda-worker (update the size)","title":"With CUDA, I receive NVMLError_NoPermission"},{"location":"install/","text":"Installation Installing with Conda (recommended) $ conda install -c conda-forge \"virtual_dataframe\" Installing with pip Use $ pip install \"virtual_dataframe\" Installing from the GitHub main branch $ pip install \"virtual_dataframe@git+https://github.com/pprados/virtual-dataframe\" Dependencies You must install all others frameworks to use it with virtual_dataframe . You can create a set of virtual environment, with the tools: $ build-conda-vdf-env --help like $ build-conda-vdf-env pandas cudf dask_cudf pyspark pyspark_gpu-local $ conda env list $ conda activate vdf-cudf $ conda activate vdf-dask_cudf-local The VDF_MODE is set for each environment. If you create an environment for a dask , spark or pyspark framework, two environment will be created. One vdf-XXX where you must set the VDF_CLUSTER variable and another vdf-XXX-local with a pre set of VDF_CLUSTER=dask://.local or VDF_CLUSTER=spark://.local to use a local cluster. For pyspark_gpu \u0300, somes environment variables will be set, to reference the rapids-4-spark_2.12-22.10.0.jar file. You must have this file in the root of your project. You can find all environement Yaml file here . You can remove all or specific versions with: $ build-conda-vdf-envs --remove","title":"Install"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#installing-with-conda-recommended","text":"$ conda install -c conda-forge \"virtual_dataframe\"","title":"Installing with Conda (recommended)"},{"location":"install/#installing-with-pip","text":"Use $ pip install \"virtual_dataframe\"","title":"Installing with pip"},{"location":"install/#installing-from-the-github-main-branch","text":"$ pip install \"virtual_dataframe@git+https://github.com/pprados/virtual-dataframe\"","title":"Installing from the GitHub main branch"},{"location":"install/#dependencies","text":"You must install all others frameworks to use it with virtual_dataframe . You can create a set of virtual environment, with the tools: $ build-conda-vdf-env --help like $ build-conda-vdf-env pandas cudf dask_cudf pyspark pyspark_gpu-local $ conda env list $ conda activate vdf-cudf $ conda activate vdf-dask_cudf-local The VDF_MODE is set for each environment. If you create an environment for a dask , spark or pyspark framework, two environment will be created. One vdf-XXX where you must set the VDF_CLUSTER variable and another vdf-XXX-local with a pre set of VDF_CLUSTER=dask://.local or VDF_CLUSTER=spark://.local to use a local cluster. For pyspark_gpu \u0300, somes environment variables will be set, to reference the rapids-4-spark_2.12-22.10.0.jar file. You must have this file in the root of your project. You can find all environement Yaml file here . You can remove all or specific versions with: $ build-conda-vdf-envs --remove","title":"Dependencies"},{"location":"tech/","text":"Technical point of view virtual_dataframe framework patch others frameworks to unify the API. VDF_MODE : Pandas like pandas cudf modin dask dask_cudf pyspark Numpy like numpy cupy dask_array Pandas like frameworks Pandas Add vdf.BackEndDataFrame = pandas.DataFrame Add vdf.BackEndSeries = pandas.Series Add vdf.BackEndArray = numpy.ndarray Add vdf.BackEndPandas = pandas Add vdf.FrontEndPandas = pandas Add vdf.FrontEndNumpy = numpy Add vdf.compute() to return a tuple of args and be compatible with dask.compute() Add vdf.concat() an alias of panda.concat() Add vdf.delayed() to delay a calland be compatible with dask.delayed() Add vdf.persist() to parameters and empty image and be compatible with dask.persist() Add vdf.visualize() to return an empty image and be compatible with dask.visualize() Add vdf.from_pandas() to return df and be compatible with `dask.from_pandas() Add vdf.from_backend() an alias of from_pandas() Add vdf.numpy an alias of numpy module Remove extra parameters used by Dask in: *.to_csv() , *.to_excel() , *.to_feather() , *.to_hdf() , *.to_json() Update the pandas API to accept glob filename in: vdf.read_csv() , vdf.read_excel() , vdf.read_feather() , vdf.read_fwf() , vdf.read_hdf , vdf.read_json() , vdf.read_orc() , vdf.read_parquet() , vdf.read_sql_table() DF.to_csv() , DF.to_excel() , DF.to_feather() , DF.to_hdf() , DF.to_json() , Series.to_csv() , Series.to_excel() , Series.to_hdf() , Series.to_json() Add methods with _not_implemented DF.to_fwf() Add DF.to_pandas() to return self Add DF.to_backend() to return self Add DF.to_ndarray() an alias to to_numpy() Add DF.apply_rows() to be compatible with cudf.apply_rows() Add DF.map_partitions() to be compatible with dask.map_partitions() Add DF.compute() to return self and be compatible with `dask.DataFrame.compute() Add DF.repartition() to return self and be compatible with `dask.DataFrame.repartition() Add DF.visualize() to return visualize(self) and be compatible with `dask.DataFrame.visualize() Add DF.categorize() to return self and be compatible with `dask.DataFrame.categorize() Add Series.to_pandas() to return self Add Series.to_backend() to return self Add Series.to_ndarray() alias of to_numpy Add Series.compute() to return self and be compatible with `dask.Series.compute() Add Series.map_partitions() to return self.map() and be compatible with `dask.Series.map_partitions() Add Series.persist() to return self and be compatible with 'dask.Series.persist() Add Series.repartition() to return self and be compatible with 'dask.Series.repartition() Add Series.visualize() to return visualize(self) and be compatible with 'dask.Series.visualize() cudf Add vdf.BackEndDataFrame = cudf.DataFrame Add vdf.BackEndSeries = cudf.Series Add vdf.BackEndArray = cupy.ndarray Add vdf.BackEndPandas = cudf Add vdf.FrontEndPandas = cudf Add vdf.FrontEndNumpy = cupy Add vdf.compute() to return an tuple of args and be compatible with dask.compute() Add vdf.concat() an alias of panda.concat() Add vdf.delayed() to delay a calland be compatible with dask.delayed() Add vdf.persist() to parameters and empty image and be compatible with dask.persist() Add vdf.visualize() to return an empty image and be compatible with dask.visualize() Add vdf.from_pandas() to return df and be compatible with `dask.from_pandas() Add vdf.from_backend() an alias of from_pandas() Add vdf.numpy an alias of cupy module Remove extra parameters used by Dask in: *.to_csv() , *.to_excel() , *.to_feather() , *.to_hdf() , *.to_json() Update the pandas API to accept glob filename in: vdf.read_csv() , vdf.read_feather() , vdf.read_json() DF.to_csv() , DF.to_excel() , DF.to_feather() , DF.to_hdf() , DF.to_json() , Series.to_hdf() , Series.to_json() Add methods with _not_implemented vdf.read_excel() , vdf.read_fwf() , vdf.read_sql_table() DF.to_csv() , DF.to_excel() Add pandas.DataFrame.to_pandas() to return self Add DF.to_backend() to return self Add DF.to_ndarray() to convert DataFrame to cupy.ndarray Add DF.map_partitions() to be compatible with dask.map_partitions() Add DF.compute() to return self and be compatible with `dask.DataFrame.compute() Add DF.repartition() to return self and be compatible with `dask.DataFrame.repartition() Add DF.visualize() to return visualize(self) and be compatible with `dask.DataFrame.visualize() Add DF.categorize() to return self and be compatible with `dask.DataFrame.categorize() Add pandas.Series.to_pandas() to return self Add Series.to_backend() to return self Add Series.to_ndarray() alias of to_numpy Add Series.compute() to return self and be compatible with `dask.Series.compute() Add Series.map_partitions() to return self.map() and be compatible with `dask.Series.map_partitions() Add Series.persist() to return self and be compatible with 'dask.Series.persist() Add Series.repartition() to return self and be compatible with 'dask.Series.repartition() Add Series.visualize() to return visualize(self) and be compatible with 'dask.Series.visualize() modin or dask_modin Set MODIN_ENGINE=dask for dask_modin Set MODIN_ENGINE=python for modin Add vdf.BackEndDataFrame = modin.pandas.DataFrame Add vdf.BackEndSeries = modin.pandas.Series Add vdf.BackEndArray = numpy.ndarray Add vdf.BackEndPandas = modin.pandas Add vdf.FrontEndPandas = modin.pandas Add vdf.FrontEndNumpy = numpy Add vdf.compute() to return a tuple of args and be compatible with dask.compute() Add vdf.concat() an alias of modin.pandas.concat() Add vdf.delayed() to delay a calland be compatible with dask.delayed() Add vdf.persist() to parameters and empty image and be compatible with dask.persist() Add vdf.visualize() to return an empty image and be compatible with dask.visualize() Add vdf.from_pandas() to return modin DataFrame or Series and be compatible with `dask.from_pandas() Add vdf.from_backend() an alias of from_pandas() Add vdf.numpy an alias of numpy module Remove extra parameters used by Dask in: *.to_csv() , *.to_excel() , *.to_feather() , *.to_hdf() , *.to_json() Add warning when using: read_excel() , read_feather() , read_fwf() , read_hdf() , read_sql_table() DF.to_excel() , DF.to_feather() , DF.to_hdf() , DF.to_sql() Series.to_csv() , Series.to_excel() , Series.to_hdf() , Series.to_json() Update the pandas API to accept glob filename in: vdf.read_excel() , vdf.read_feather() , vdf.read_fwf() , vdf.read_hdf , vdf.read_orc() DF.to_excel() , DF.to_feather() , DF.to_hdf() , DF.to_sql() Series.to_csv() , Series.to_excel() , Series.to_hdf() , Series.to_json() Add methods with _not_implemented DF.to_orc() Add DF.to_pandas() to convert to panda.DataFrame Add DF.to_backend() to return self Add DF.to_ndarray() an alias to to_numpy() Add DF.apply_rows() to be compatible with cudf.apply_rows() Add DF.map_partitions() to be compatible with dask.map_partitions() Add DF.compute() to return self and be compatible with `dask.DataFrame.compute() Add DF.repartition() to return self and be compatible with `dask.DataFrame.repartition() Add DF.visualize() to return visualize(self) and be compatible with `dask.DataFrame.visualize() Add DF.categorize() to return self and be compatible with `dask.DataFrame.categorize() Add Series.to_pandas() to return modin.pandas.Series.to_pandas() Add Series.to_backend() to return self Add Series.to_ndarray() alias of to_numpy Add Series.compute() to return self and be compatible with `dask.Series.compute() Add Series.map_partitions() to return self.map() and be compatible with `dask.Series.map_partitions() Add Series.persist() to return self and be compatible with 'dask.Series.persist() Add Series.repartition() to return self and be compatible with 'dask.Series.repartition() Add Series.visualize() to return visualize(self) and be compatible with 'dask.Series.visualize() And all patch in pandas dask Add vdf.BackEndDataFrame = pandas.DataFrame Add vdf.BackEndSeries = pandas.Series Add vdf.BackEndArray = numpy.ndarray Add vdf.BackEndPandas = pandas Add vdf.FrontEndPandas = dask.dataframe Add vdf.FrontEndNumpy = dask.array Add vdf.concat() an alias of dask.dataframe.multi.concat() Add vdf.from_pandas() an alias of dask.dataframe.from_pandas() Add vdf.from_backend() an alias of from_pandas() Add vdf.numpy an alias of numpy module Add warning in: read_fwf() , read_hdf() , read_sql_table() Add methods with _not_implemented read_excel() , read_feather() DF.to_excel() , DF.to_feather() , DF.to_fwf() Add DF.to_pandas() to return self.compute() Add DF.to_backend() an alias of to_pandas() Add DF.to_numpy() to return self.compute().to_numpy() Add DF.to_ndarray() an alias to dask.DataFrame.to_dask_array() Add DF.apply_rows() to be compatible with cudf.apply_rows() Patch DF.to_sql() and Series.to_sql() to accept con or uri Add Series.to_pandas() to return self.compute() Add Series.to_backend() an alias of to_pandas() Add Series.to_numpy() to return self.compute().to_numpy() Add Series.to_ndarray() alias of dask.dataframe.Series.to_dask_array() And all patch in pandas dask_cudf Add vdf.BackEndDataFrame = cudf.DataFrame Add vdf.BackEndSeries = cudf.Series Add vdf.BackEndArray = cudf Add vdf.BackEndPandas = pandas Add vdf.FrontEndPandas = dask_cudf Add vdf.FrontEndNumpy = cupy Add vdf.compute() to dask.compute() Add vdf.concat() to dask.dataframe.multi.concat() Add vdf.delayed() to dask.delayed() Add vdf.persist() to dask.persist() Add vdf.visualize() to dask.visualize() Add vdf.from_pandas() to dask_cudf.from_cudf() Add vdf.from_backend() to dask_cudf.from_cudf() Add vdf.numpy an alias of cupy module Add a warning in: Series.to_hdf() , Series.to_json() Add methods with _not_implemented read_excel() , read_feather() , read_fwf() , read_hdf() , read_sql_table() DF.to_excel() , DF.to_feather() , DF.to_fwf() , DF.to_hdf() , DF.to_sql() , Series.to_csv() , Series.to_excel() , Add DF.to_pandas() to return self.compute().to_pandas() Add DF.to_backend() to return self.compute() and return cudf.DataFrame Add DF.to_numpy() to self.compute().to_numpy() Add DF.to_ndarray() an alias to self.compute() and return cudf.DataFrame Add Series.to_pandas() to return self.compute().to_pandas() Add Series.to_backend() to return self.compute() and return cudf.Series Add Series.to_numpy() to return self.compute().to_numpy() Add Series.to_ndarray() to return a cudf.Series Add Series.compute() to return self and be compatible with `dask.Series.compute() Add Series.map_partitions() to return self.map() and be compatible with `dask.Series.map_partitions() Add Series.persist() to return self and be compatible with 'dask.Series.persist() Add Series.repartition() to return self and be compatible with 'dask.Series.repartition() Add Series.visualize() to return visualize(self) and be compatible with 'dask.Series.visualize() And all patch in cudf pyspark Add vdf.BackEndDataFrame = pandas.DataFrame Add vdf.BackEndSeries = pandas.Series Add vdf.BackEndArray = numpy.ndarray Add vdf.BackEndPandas = pandas Add vdf.FrontEndPandas = pyspark.pandas Add vdf.FrontEndNumpy = numpy Add vdf.compute() to return a tuple of args and be compatible with dask.compute() Add vdf.concat() an alias of pyspark.pandas.concat() Add vdf.delayed() to delay a call and be compatible with dask.delayed() Add vdf.persist() to persist the current DF Add vdf.visualize() to return an empty image and be compatible with dask.visualize() Add vdf.from_backend() an alias of from_pandas() Add vdf.numpy an alias of numpy module Remove extra parameters used by Dask in: *.to_csv() , *.to_excel() , *.to_feather() , *.to_hdf() , *.to_json() from_pandas() Add warning in: read_excel() , reql_sql_table() Update the pandas API to accept glob filename in: vdf.read_csv() , vdf.read_excel() , vdf.read_json() , vdf.read_orc() DF.to_csv() , DF.to_excel() , DF.to_feather() , DF.to_hdf() , DF.to_json() , Series.to_csv() , Series.to_excel() , Series.to_hdf() , Series.to_json() Add methods with _not_implemented vdf.read_feather() , vdf.read_fwf() , vdf.read_hdf() DF.to_sql() , Series.to_sql() Add DF.to_backend() an alias of to_pandas() Add DF.to_ndarray() an alias to to_numpy() Add DF.apply_rows() to be compatible with cudf.apply_rows() Add DF.categorize() to return self and be compatible with `dask.DataFrame.categorize() Add DF.compute() to return self and be compatible with `dask.DataFrame.compute() Add DF.map_partitions() to be compatible with dask.map_partitions() Add DF.persist() to return self and be compatible with `dask.DataFrame.visualize() Add DF.repartition() to return self and be compatible with `dask.DataFrame.repartition() Add DF.visualize() to return visualize(self) and be compatible with `dask.DataFrame.visualize() Add Series.to_backend() alias of to_pandas() Add Series.to_ndarray() alias of to_numpy() Add Series.compute() to return self and be compatible with `dask.Series.compute() Add Series.map_partitions() to return self.map() and be compatible with `dask.Series.map_partitions() Add Series.persist() to return self and be compatible with 'dask.Series.persist() Add Series.repartition() to return self and be compatible with 'dask.Series.repartition() Add Series.visualize() to return visualize(self) and be compatible with 'dask.Series.visualize() Numpy like familly Numpy It's not possible to update some method in numpy.ndarray . vdf.numpy is an alias of numpy Add vdf.numpy.asnumpy(ar) to return ar Add vdf.numpy.asndarray(ar) to return ar.to_numpy() Add vdf.numpy.compute(...) to return a tuple with parameters Add vdf.numpy.compute_chunk_sizes(ar) to return ar Add vdf.numpy.rechunk(ar) to return ar Add vdf.numpy.arange() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.from_array() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.load() to remove the parameter chunks Add vdf.numpy.save() to remove the parameter chunks Add vdf.numpy.savez() to remove the parameter chunks Add vdf.numpy.random.* to remove the parameter chunks cupy vdf.numpy is an alias of cupy Add vdf.numpy.asndarray(ar) to return ar.to_numpy() Add vdf.numpy.compute(...) to return a tuple with parameters Add vdf.numpy.compute_chunk_sizes(ar) to return ar Add vdf.numpy.rechunk(ar) to return ar Add vdf.numpy.arange() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.from_array() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.load() to remove the parameter chunks Add vdf.numpy.save() to remove the parameter chunks Add vdf.numpy.savez() to remove the parameter chunks Add vdf.numpy.random.* to remove the parameter chunks dask_array vdf.numpy is an alias of dasl.array Add vdf.numpy.asarray(ar) to return array of numpy or cupy Add vdf.numpy.asndarray(ar) to return ar.to_numpy() Add vdf.numpy.compute(...) to return a tuple with parameters Add vdf.numpy.compute_chunk_sizes(ar) to return ar Add vdf.numpy.rechunk(ar) to return ar Add vdf.numpy.arange() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.from_array() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.load() to remove the parameter chunks Add vdf.numpy.save() to remove the parameter chunks Add vdf.numpy.savez() to remove the parameter chunks Add vdf.numpy.random.* to remove the parameter chunks","title":"Tech"},{"location":"tech/#technical-point-of-view","text":"virtual_dataframe framework patch others frameworks to unify the API. VDF_MODE : Pandas like pandas cudf modin dask dask_cudf pyspark Numpy like numpy cupy dask_array","title":"Technical point of view"},{"location":"tech/#pandas-like-frameworks","text":"","title":"Pandas like frameworks"},{"location":"tech/#pandas","text":"Add vdf.BackEndDataFrame = pandas.DataFrame Add vdf.BackEndSeries = pandas.Series Add vdf.BackEndArray = numpy.ndarray Add vdf.BackEndPandas = pandas Add vdf.FrontEndPandas = pandas Add vdf.FrontEndNumpy = numpy Add vdf.compute() to return a tuple of args and be compatible with dask.compute() Add vdf.concat() an alias of panda.concat() Add vdf.delayed() to delay a calland be compatible with dask.delayed() Add vdf.persist() to parameters and empty image and be compatible with dask.persist() Add vdf.visualize() to return an empty image and be compatible with dask.visualize() Add vdf.from_pandas() to return df and be compatible with `dask.from_pandas() Add vdf.from_backend() an alias of from_pandas() Add vdf.numpy an alias of numpy module Remove extra parameters used by Dask in: *.to_csv() , *.to_excel() , *.to_feather() , *.to_hdf() , *.to_json() Update the pandas API to accept glob filename in: vdf.read_csv() , vdf.read_excel() , vdf.read_feather() , vdf.read_fwf() , vdf.read_hdf , vdf.read_json() , vdf.read_orc() , vdf.read_parquet() , vdf.read_sql_table() DF.to_csv() , DF.to_excel() , DF.to_feather() , DF.to_hdf() , DF.to_json() , Series.to_csv() , Series.to_excel() , Series.to_hdf() , Series.to_json() Add methods with _not_implemented DF.to_fwf() Add DF.to_pandas() to return self Add DF.to_backend() to return self Add DF.to_ndarray() an alias to to_numpy() Add DF.apply_rows() to be compatible with cudf.apply_rows() Add DF.map_partitions() to be compatible with dask.map_partitions() Add DF.compute() to return self and be compatible with `dask.DataFrame.compute() Add DF.repartition() to return self and be compatible with `dask.DataFrame.repartition() Add DF.visualize() to return visualize(self) and be compatible with `dask.DataFrame.visualize() Add DF.categorize() to return self and be compatible with `dask.DataFrame.categorize() Add Series.to_pandas() to return self Add Series.to_backend() to return self Add Series.to_ndarray() alias of to_numpy Add Series.compute() to return self and be compatible with `dask.Series.compute() Add Series.map_partitions() to return self.map() and be compatible with `dask.Series.map_partitions() Add Series.persist() to return self and be compatible with 'dask.Series.persist() Add Series.repartition() to return self and be compatible with 'dask.Series.repartition() Add Series.visualize() to return visualize(self) and be compatible with 'dask.Series.visualize()","title":"Pandas"},{"location":"tech/#cudf","text":"Add vdf.BackEndDataFrame = cudf.DataFrame Add vdf.BackEndSeries = cudf.Series Add vdf.BackEndArray = cupy.ndarray Add vdf.BackEndPandas = cudf Add vdf.FrontEndPandas = cudf Add vdf.FrontEndNumpy = cupy Add vdf.compute() to return an tuple of args and be compatible with dask.compute() Add vdf.concat() an alias of panda.concat() Add vdf.delayed() to delay a calland be compatible with dask.delayed() Add vdf.persist() to parameters and empty image and be compatible with dask.persist() Add vdf.visualize() to return an empty image and be compatible with dask.visualize() Add vdf.from_pandas() to return df and be compatible with `dask.from_pandas() Add vdf.from_backend() an alias of from_pandas() Add vdf.numpy an alias of cupy module Remove extra parameters used by Dask in: *.to_csv() , *.to_excel() , *.to_feather() , *.to_hdf() , *.to_json() Update the pandas API to accept glob filename in: vdf.read_csv() , vdf.read_feather() , vdf.read_json() DF.to_csv() , DF.to_excel() , DF.to_feather() , DF.to_hdf() , DF.to_json() , Series.to_hdf() , Series.to_json() Add methods with _not_implemented vdf.read_excel() , vdf.read_fwf() , vdf.read_sql_table() DF.to_csv() , DF.to_excel() Add pandas.DataFrame.to_pandas() to return self Add DF.to_backend() to return self Add DF.to_ndarray() to convert DataFrame to cupy.ndarray Add DF.map_partitions() to be compatible with dask.map_partitions() Add DF.compute() to return self and be compatible with `dask.DataFrame.compute() Add DF.repartition() to return self and be compatible with `dask.DataFrame.repartition() Add DF.visualize() to return visualize(self) and be compatible with `dask.DataFrame.visualize() Add DF.categorize() to return self and be compatible with `dask.DataFrame.categorize() Add pandas.Series.to_pandas() to return self Add Series.to_backend() to return self Add Series.to_ndarray() alias of to_numpy Add Series.compute() to return self and be compatible with `dask.Series.compute() Add Series.map_partitions() to return self.map() and be compatible with `dask.Series.map_partitions() Add Series.persist() to return self and be compatible with 'dask.Series.persist() Add Series.repartition() to return self and be compatible with 'dask.Series.repartition() Add Series.visualize() to return visualize(self) and be compatible with 'dask.Series.visualize()","title":"cudf"},{"location":"tech/#modin-or-dask_modin","text":"Set MODIN_ENGINE=dask for dask_modin Set MODIN_ENGINE=python for modin Add vdf.BackEndDataFrame = modin.pandas.DataFrame Add vdf.BackEndSeries = modin.pandas.Series Add vdf.BackEndArray = numpy.ndarray Add vdf.BackEndPandas = modin.pandas Add vdf.FrontEndPandas = modin.pandas Add vdf.FrontEndNumpy = numpy Add vdf.compute() to return a tuple of args and be compatible with dask.compute() Add vdf.concat() an alias of modin.pandas.concat() Add vdf.delayed() to delay a calland be compatible with dask.delayed() Add vdf.persist() to parameters and empty image and be compatible with dask.persist() Add vdf.visualize() to return an empty image and be compatible with dask.visualize() Add vdf.from_pandas() to return modin DataFrame or Series and be compatible with `dask.from_pandas() Add vdf.from_backend() an alias of from_pandas() Add vdf.numpy an alias of numpy module Remove extra parameters used by Dask in: *.to_csv() , *.to_excel() , *.to_feather() , *.to_hdf() , *.to_json() Add warning when using: read_excel() , read_feather() , read_fwf() , read_hdf() , read_sql_table() DF.to_excel() , DF.to_feather() , DF.to_hdf() , DF.to_sql() Series.to_csv() , Series.to_excel() , Series.to_hdf() , Series.to_json() Update the pandas API to accept glob filename in: vdf.read_excel() , vdf.read_feather() , vdf.read_fwf() , vdf.read_hdf , vdf.read_orc() DF.to_excel() , DF.to_feather() , DF.to_hdf() , DF.to_sql() Series.to_csv() , Series.to_excel() , Series.to_hdf() , Series.to_json() Add methods with _not_implemented DF.to_orc() Add DF.to_pandas() to convert to panda.DataFrame Add DF.to_backend() to return self Add DF.to_ndarray() an alias to to_numpy() Add DF.apply_rows() to be compatible with cudf.apply_rows() Add DF.map_partitions() to be compatible with dask.map_partitions() Add DF.compute() to return self and be compatible with `dask.DataFrame.compute() Add DF.repartition() to return self and be compatible with `dask.DataFrame.repartition() Add DF.visualize() to return visualize(self) and be compatible with `dask.DataFrame.visualize() Add DF.categorize() to return self and be compatible with `dask.DataFrame.categorize() Add Series.to_pandas() to return modin.pandas.Series.to_pandas() Add Series.to_backend() to return self Add Series.to_ndarray() alias of to_numpy Add Series.compute() to return self and be compatible with `dask.Series.compute() Add Series.map_partitions() to return self.map() and be compatible with `dask.Series.map_partitions() Add Series.persist() to return self and be compatible with 'dask.Series.persist() Add Series.repartition() to return self and be compatible with 'dask.Series.repartition() Add Series.visualize() to return visualize(self) and be compatible with 'dask.Series.visualize() And all patch in pandas","title":"modin or dask_modin"},{"location":"tech/#dask","text":"Add vdf.BackEndDataFrame = pandas.DataFrame Add vdf.BackEndSeries = pandas.Series Add vdf.BackEndArray = numpy.ndarray Add vdf.BackEndPandas = pandas Add vdf.FrontEndPandas = dask.dataframe Add vdf.FrontEndNumpy = dask.array Add vdf.concat() an alias of dask.dataframe.multi.concat() Add vdf.from_pandas() an alias of dask.dataframe.from_pandas() Add vdf.from_backend() an alias of from_pandas() Add vdf.numpy an alias of numpy module Add warning in: read_fwf() , read_hdf() , read_sql_table() Add methods with _not_implemented read_excel() , read_feather() DF.to_excel() , DF.to_feather() , DF.to_fwf() Add DF.to_pandas() to return self.compute() Add DF.to_backend() an alias of to_pandas() Add DF.to_numpy() to return self.compute().to_numpy() Add DF.to_ndarray() an alias to dask.DataFrame.to_dask_array() Add DF.apply_rows() to be compatible with cudf.apply_rows() Patch DF.to_sql() and Series.to_sql() to accept con or uri Add Series.to_pandas() to return self.compute() Add Series.to_backend() an alias of to_pandas() Add Series.to_numpy() to return self.compute().to_numpy() Add Series.to_ndarray() alias of dask.dataframe.Series.to_dask_array() And all patch in pandas","title":"dask"},{"location":"tech/#dask_cudf","text":"Add vdf.BackEndDataFrame = cudf.DataFrame Add vdf.BackEndSeries = cudf.Series Add vdf.BackEndArray = cudf Add vdf.BackEndPandas = pandas Add vdf.FrontEndPandas = dask_cudf Add vdf.FrontEndNumpy = cupy Add vdf.compute() to dask.compute() Add vdf.concat() to dask.dataframe.multi.concat() Add vdf.delayed() to dask.delayed() Add vdf.persist() to dask.persist() Add vdf.visualize() to dask.visualize() Add vdf.from_pandas() to dask_cudf.from_cudf() Add vdf.from_backend() to dask_cudf.from_cudf() Add vdf.numpy an alias of cupy module Add a warning in: Series.to_hdf() , Series.to_json() Add methods with _not_implemented read_excel() , read_feather() , read_fwf() , read_hdf() , read_sql_table() DF.to_excel() , DF.to_feather() , DF.to_fwf() , DF.to_hdf() , DF.to_sql() , Series.to_csv() , Series.to_excel() , Add DF.to_pandas() to return self.compute().to_pandas() Add DF.to_backend() to return self.compute() and return cudf.DataFrame Add DF.to_numpy() to self.compute().to_numpy() Add DF.to_ndarray() an alias to self.compute() and return cudf.DataFrame Add Series.to_pandas() to return self.compute().to_pandas() Add Series.to_backend() to return self.compute() and return cudf.Series Add Series.to_numpy() to return self.compute().to_numpy() Add Series.to_ndarray() to return a cudf.Series Add Series.compute() to return self and be compatible with `dask.Series.compute() Add Series.map_partitions() to return self.map() and be compatible with `dask.Series.map_partitions() Add Series.persist() to return self and be compatible with 'dask.Series.persist() Add Series.repartition() to return self and be compatible with 'dask.Series.repartition() Add Series.visualize() to return visualize(self) and be compatible with 'dask.Series.visualize() And all patch in cudf","title":"dask_cudf"},{"location":"tech/#pyspark","text":"Add vdf.BackEndDataFrame = pandas.DataFrame Add vdf.BackEndSeries = pandas.Series Add vdf.BackEndArray = numpy.ndarray Add vdf.BackEndPandas = pandas Add vdf.FrontEndPandas = pyspark.pandas Add vdf.FrontEndNumpy = numpy Add vdf.compute() to return a tuple of args and be compatible with dask.compute() Add vdf.concat() an alias of pyspark.pandas.concat() Add vdf.delayed() to delay a call and be compatible with dask.delayed() Add vdf.persist() to persist the current DF Add vdf.visualize() to return an empty image and be compatible with dask.visualize() Add vdf.from_backend() an alias of from_pandas() Add vdf.numpy an alias of numpy module Remove extra parameters used by Dask in: *.to_csv() , *.to_excel() , *.to_feather() , *.to_hdf() , *.to_json() from_pandas() Add warning in: read_excel() , reql_sql_table() Update the pandas API to accept glob filename in: vdf.read_csv() , vdf.read_excel() , vdf.read_json() , vdf.read_orc() DF.to_csv() , DF.to_excel() , DF.to_feather() , DF.to_hdf() , DF.to_json() , Series.to_csv() , Series.to_excel() , Series.to_hdf() , Series.to_json() Add methods with _not_implemented vdf.read_feather() , vdf.read_fwf() , vdf.read_hdf() DF.to_sql() , Series.to_sql() Add DF.to_backend() an alias of to_pandas() Add DF.to_ndarray() an alias to to_numpy() Add DF.apply_rows() to be compatible with cudf.apply_rows() Add DF.categorize() to return self and be compatible with `dask.DataFrame.categorize() Add DF.compute() to return self and be compatible with `dask.DataFrame.compute() Add DF.map_partitions() to be compatible with dask.map_partitions() Add DF.persist() to return self and be compatible with `dask.DataFrame.visualize() Add DF.repartition() to return self and be compatible with `dask.DataFrame.repartition() Add DF.visualize() to return visualize(self) and be compatible with `dask.DataFrame.visualize() Add Series.to_backend() alias of to_pandas() Add Series.to_ndarray() alias of to_numpy() Add Series.compute() to return self and be compatible with `dask.Series.compute() Add Series.map_partitions() to return self.map() and be compatible with `dask.Series.map_partitions() Add Series.persist() to return self and be compatible with 'dask.Series.persist() Add Series.repartition() to return self and be compatible with 'dask.Series.repartition() Add Series.visualize() to return visualize(self) and be compatible with 'dask.Series.visualize()","title":"pyspark"},{"location":"tech/#numpy-like-familly","text":"","title":"Numpy like familly"},{"location":"tech/#numpy","text":"It's not possible to update some method in numpy.ndarray . vdf.numpy is an alias of numpy Add vdf.numpy.asnumpy(ar) to return ar Add vdf.numpy.asndarray(ar) to return ar.to_numpy() Add vdf.numpy.compute(...) to return a tuple with parameters Add vdf.numpy.compute_chunk_sizes(ar) to return ar Add vdf.numpy.rechunk(ar) to return ar Add vdf.numpy.arange() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.from_array() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.load() to remove the parameter chunks Add vdf.numpy.save() to remove the parameter chunks Add vdf.numpy.savez() to remove the parameter chunks Add vdf.numpy.random.* to remove the parameter chunks","title":"Numpy"},{"location":"tech/#cupy","text":"vdf.numpy is an alias of cupy Add vdf.numpy.asndarray(ar) to return ar.to_numpy() Add vdf.numpy.compute(...) to return a tuple with parameters Add vdf.numpy.compute_chunk_sizes(ar) to return ar Add vdf.numpy.rechunk(ar) to return ar Add vdf.numpy.arange() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.from_array() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.load() to remove the parameter chunks Add vdf.numpy.save() to remove the parameter chunks Add vdf.numpy.savez() to remove the parameter chunks Add vdf.numpy.random.* to remove the parameter chunks","title":"cupy"},{"location":"tech/#dask_array","text":"vdf.numpy is an alias of dasl.array Add vdf.numpy.asarray(ar) to return array of numpy or cupy Add vdf.numpy.asndarray(ar) to return ar.to_numpy() Add vdf.numpy.compute(...) to return a tuple with parameters Add vdf.numpy.compute_chunk_sizes(ar) to return ar Add vdf.numpy.rechunk(ar) to return ar Add vdf.numpy.arange() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.from_array() , remove the parameter chunks , invoke numpy.arange() and return a view with Vndarray Add vdf.numpy.load() to remove the parameter chunks Add vdf.numpy.save() to remove the parameter chunks Add vdf.numpy.savez() to remove the parameter chunks Add vdf.numpy.random.* to remove the parameter chunks","title":"dask_array"},{"location":"test/","text":"Test To test your code with all scenario, you can write a Makefile : # Makefile export VDF_MODES=pandas cudf dask dask_modin dask_cudf pyspark export VDF_MODE # Run unit test with a specific *mode* .PHONY: unit-test-* .make-_unit-test-%: $(REQUIREMENTS) $(PYTHON_TST) $(PYTHON_SRC) @$(VALIDATE_VENV) echo \"Run unit tests...\" python -m pytest --rootdir=. -s tests date >.make-_unit-test-$* unit-test-%: @echo \"Test with VDF_MODE=$*\" VDF_MODE=$* $(MAKE) --no-print-directory .make-_unit-test-$* unit-test: $(foreach ext,$(VDF_MODES),unit-test-$(ext)) then make unit-test-pandas # Test only pandas make unit-test-pyspark # Test only pyspark make unit-test # Test all scenario","title":"Test"},{"location":"test/#test","text":"To test your code with all scenario, you can write a Makefile : # Makefile export VDF_MODES=pandas cudf dask dask_modin dask_cudf pyspark export VDF_MODE # Run unit test with a specific *mode* .PHONY: unit-test-* .make-_unit-test-%: $(REQUIREMENTS) $(PYTHON_TST) $(PYTHON_SRC) @$(VALIDATE_VENV) echo \"Run unit tests...\" python -m pytest --rootdir=. -s tests date >.make-_unit-test-$* unit-test-%: @echo \"Test with VDF_MODE=$*\" VDF_MODE=$* $(MAKE) --no-print-directory .make-_unit-test-$* unit-test: $(foreach ext,$(VDF_MODES),unit-test-$(ext)) then make unit-test-pandas # Test only pandas make unit-test-pyspark # Test only pyspark make unit-test # Test all scenario","title":"Test"}]}